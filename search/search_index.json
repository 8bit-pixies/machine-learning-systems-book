{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Systems Many modern machine learning systems require an enormous amount of infrastructure setup and expensive compute resources. This book aims to explore how machine learning models can be built and deployed in low resource settings, without making assumptions that a cloud provider is available, the availability of message queues or even Kubernetes. By reading this book, I hope it encourages readers to push the boundaries of how software applications can be written in the machine learning contexts leading to simplier deployments which hopefully leads to easier management of data pipelines and processes. Of course this book will offer an opinionated take - though my hope is that the reader will gain new perspective and strong grounding in machine learning systems to scale to more complex frameworks. Thank you so much for reading.","title":"Machine Learning Systems"},{"location":"#machine-learning-systems","text":"Many modern machine learning systems require an enormous amount of infrastructure setup and expensive compute resources. This book aims to explore how machine learning models can be built and deployed in low resource settings, without making assumptions that a cloud provider is available, the availability of message queues or even Kubernetes. By reading this book, I hope it encourages readers to push the boundaries of how software applications can be written in the machine learning contexts leading to simplier deployments which hopefully leads to easier management of data pipelines and processes. Of course this book will offer an opinionated take - though my hope is that the reader will gain new perspective and strong grounding in machine learning systems to scale to more complex frameworks. Thank you so much for reading.","title":"Machine Learning Systems"},{"location":"introduction/","text":"Introduction Welcome to my Machine Learning Systems Book , which is all about well...machine learning systems! Who This Book Is For This book explores generic patterns and recipes for productionising machine learning. It focuses primarily on low resources and low infrastructure setups, particularly pertaining how you might train and deploy complex machine learning systems in relatively constrained settings. This book is my unofficial response to the \"Machine Learning Design Patterns\", which I personally felt focussed heavily on building machine learning systems for large enterprise teams - which unfortauntely is not always the case. Machine learning should be possible for all, not just organisations who have the capability to hire large teams. How to Use This Book In general we assume the book is read sequentially, but of course feel free to skip ahead, and jump back - whatever works for you!","title":"Introduction"},{"location":"introduction/#introduction","text":"Welcome to my Machine Learning Systems Book , which is all about well...machine learning systems!","title":"Introduction"},{"location":"introduction/#who-this-book-is-for","text":"This book explores generic patterns and recipes for productionising machine learning. It focuses primarily on low resources and low infrastructure setups, particularly pertaining how you might train and deploy complex machine learning systems in relatively constrained settings. This book is my unofficial response to the \"Machine Learning Design Patterns\", which I personally felt focussed heavily on building machine learning systems for large enterprise teams - which unfortauntely is not always the case. Machine learning should be possible for all, not just organisations who have the capability to hire large teams.","title":"Who This Book Is For"},{"location":"introduction/#how-to-use-this-book","text":"In general we assume the book is read sequentially, but of course feel free to skip ahead, and jump back - whatever works for you!","title":"How to Use This Book"},{"location":"1.%20Getting%20Started/01-installation/","text":"Installation The first step for working with Python is installing Python itself. The easiest way is to install Python via command line tools. Command Line Notation In this chapter and throughout the book, we\u2019ll show some commands used in the terminal. Lines that you should enter in a terminal all start with $ . You don\u2019t need to type in the $ character; it indicates the start of each command. Lines that don\u2019t start with $ typically show the output of the previous command. Additionally, PowerShell-specific examples will use > rather than $ . Installing Python on macOS If you are using macOS and you have brew installed, then you can enter the following command: $ brew install python Conventions Used In this book, we will focus on the following libraries and their APIs: scikit-learn : for building machine learning models and pipelines For introducing ideas around transfer learning, we will use the following libraries - and their pre-built models: transformers : the huggingface library keras : the deep learning library Who Productionises Machine Learning? In small organisations, the person who would build a model would productionise the model! Based on this you might wonder \"how would we scale this\"? By keeping machine learning systems simple, it becomes easier to scale and manage. Of course in smaller organisations, you also wouldn't necessarily have \"big data\" challenges, your data would only be moderately sized! Nevertheless, we will touch on these topics within this book.","title":"Installation and Getting Started"},{"location":"1.%20Getting%20Started/01-installation/#installation","text":"The first step for working with Python is installing Python itself. The easiest way is to install Python via command line tools.","title":"Installation"},{"location":"1.%20Getting%20Started/01-installation/#command-line-notation","text":"In this chapter and throughout the book, we\u2019ll show some commands used in the terminal. Lines that you should enter in a terminal all start with $ . You don\u2019t need to type in the $ character; it indicates the start of each command. Lines that don\u2019t start with $ typically show the output of the previous command. Additionally, PowerShell-specific examples will use > rather than $ .","title":"Command Line Notation"},{"location":"1.%20Getting%20Started/01-installation/#installing-python-on-macos","text":"If you are using macOS and you have brew installed, then you can enter the following command: $ brew install python","title":"Installing Python on macOS"},{"location":"1.%20Getting%20Started/01-installation/#conventions-used","text":"In this book, we will focus on the following libraries and their APIs: scikit-learn : for building machine learning models and pipelines For introducing ideas around transfer learning, we will use the following libraries - and their pre-built models: transformers : the huggingface library keras : the deep learning library","title":"Conventions Used"},{"location":"1.%20Getting%20Started/01-installation/#who-productionises-machine-learning","text":"In small organisations, the person who would build a model would productionise the model! Based on this you might wonder \"how would we scale this\"? By keeping machine learning systems simple, it becomes easier to scale and manage. Of course in smaller organisations, you also wouldn't necessarily have \"big data\" challenges, your data would only be moderately sized! Nevertheless, we will touch on these topics within this book.","title":"Who Productionises Machine Learning?"},{"location":"2.%20Data%20Representation/01-finding-rules/","text":"The first stage in feature engineering is to determine a set of optimal rules. The typical workflow uses techniques such as: Finding correlations Performing EDA Bucketing values Determining features which are highly correlated Building Discretized Features Using scikit-learn we can build discretized features quickly and effectively. from sklearn.model_selection import train_test_split from sklearn import datasets from sklearn.preprocessing import KBinsDiscretizer X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) kbins = KBinsDiscretizer ( n_bins = 20 ) kbins . fit ( X_train , y_train ) output = kbins . transform ( X_train ) # shape: (None, 169) Optimal using Feature Selection Even in trivial datasets, the number of features increases exponentially, we can aggressively reduce the feature space, even when taking into account. A common approach is to expand the feature space and add a feature selection component in a pipeline. from sklearn.model_selection import train_test_split from sklearn import datasets , svm from sklearn.preprocessing import KBinsDiscretizer , PolynomialFeatures from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectFromModel X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) feature_pipeline = make_pipeline ( PolynomialFeatures ( degree = 2 , interaction_only = True ), KBinsDiscretizer ( n_bins = 256 , strategy = 'uniform' ), SelectFromModel ( svm . LinearSVC ( penalty = \"l1\" , dual = False ), max_features = 512 ) # without this we'll have over 2,000 features ) feature_pipeline . fit ( X_train , y_train ) output = feature_pipeline . transform ( X_train ) # shape: (None, 512) This approach can learn to select features based on linear assumptions. For more background on feature selection in the online setting, you can read more on Grafting. (Bonus) Optimal Discretized Buckets using RuleFit Approaches such as RuleFit will automatically find candidate features as part of its model building process. As it automatically determines interactions and optimises the feature space. To begin we need to install the interpret package which includes wrappers for RuleFit and the official implementation for Explainable Boosting Machines. import pandas as pd from sklearn.model_selection import train_test_split from sklearn.base import TransformerMixin import numpy as np from interpret.glassbox import ExplainableBoostingRegressor from interpret import show X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) ebm = ExplainableBoostingRegressor ( random_state = seed ) ebm . fit ( X_train , y_train ) ebm_global = ebm . explain_global () graph_data = ebm_global . data ( 0 ) # contains information on the feature and confidence interval # for features which use interactions, it looks in all subsets and performs a table lookup (not implemented) class BoostedRegressorBinsDiscretizer ( TransformerMixin ): def __init__ ( self , merge_bins = True , ebm_config = {}): self . ebm_config = ebm_config self . ebm = None self . merge_bins = merge_bins def fit ( self , X , y , ** kwargs ): # ..omitted.. def transform ( self , X ): # ..omitted.. bbd = BoostedRegressorBinsDiscretizer () bbd . fit ( X_train , y_train ) bbd . transform ( X_train ) This constructs a \"smarter\" binning approach than doing it unsupervised (or is a bit bitter than performing it unsupervised followed by feature selection). By performing a table-based lookup when considering interaction terms is the same as constructing a learned embedding over a particular space. This is an efficient way (albeit offline) construction which can help determine non-linearities for used with a linear machine learning model (e.g. as used in Explainable Boosting Machines). Should we use this code for production? Personally, a custom transformers which wraps around an existing model is a bit risky, as having custom code means needing to maintain custom packages. Perhaps building custom pipelines is preferable rather than ephemeral wrappers which are not officially supported. Building pipelines with appropriate metadata interfaces is probably preferable in the longer term. An improved interface could be: import pandas as pd import numpy as np def build_config ( model ): \"\"\" See reference: https://github.com/interpretml/ebm2onnx/blob/master/ebm2onnx/convert.py \"\"\" model_config = [] model_global = model . explain_global () for feature_index in range ( len ( model . feature_names )): feature_name = model . feature_names [ feature_index ] feature_type = model . feature_types [ feature_index ] feature_group = model . feature_groups_ [ feature_index ] info_config = {} if feature_type == \"continuous\" : info_config [ \"feature_type\" ] = feature_type info_config [ \"column_name\" ] = [ feature_name ] info_config [ \"column_index\" ] = [ feature_group [ 0 ]] info_config [ \"column_mapping\" ] = ( [ - np . inf ] + list ( model . preprocessor_ . col_bin_edges_ [ feature_group [ 0 ]]) + [ np . inf ] ) info_config [ \"scores\" ] = model . additive_terms_ [ feature_index ][ 1 :] # print(len(list(pd.IntervalIndex.from_tuples(list(zip(info_config[\"column_mapping\"][0][:-1], info_config[\"column_mapping\"][0][1:]))))), # len(info_config['scores'])) info_config [ \"table\" ] = pd . DataFrame ( { \"interval\" : list ( pd . IntervalIndex . from_tuples ( list ( zip ( info_config [ \"column_mapping\" ][ 0 ][: - 1 ], info_config [ \"column_mapping\" ][ 0 ][ 1 :], ) ) ) ), \"scores\" : info_config [ \"scores\" ], } ) elif feature_type == \"categorical\" : info_config [ \"feature_type\" ] = feature_type info_config [ \"column_name\" ] = [ feature_name ] info_config [ \"column_index\" ] = [ feature_group [ 0 ]] info_config [ \"column_mapping\" ] = model . preprocessor_ . col_mapping_ [ feature_group [ 0 ] ] info_config [ \"scores\" ] = model . additive_terms_ [ feature_index ] row_index = list ( info_config [ \"column_mapping\" ] . keys ()) dummy_index = \" \" while dummy_index in row_index : dummy_index += \" \" row_index = [ dummy_index ] + row_index info_config [ \"table\" ] = pd . DataFrame ( { \"categories\" : row_index , \"scores\" : info_config [ \"scores\" ]} ) elif feature_type == \"interaction\" : # left part right part? I think using range is harder to read - maybe. info_config [ \"feature_type\" ] = [ model . feature_types [ idx ] for idx in feature_group ] info_config [ \"column_name\" ] = [ model . preprocessor_ . feature_names [ idx ] for idx in feature_group ] info_config [ \"column_index\" ] = list ( feature_group ) if info_config [ \"feature_type\" ][ 0 ] == \"continuous\" : left_mapping = ( [ - np . inf ] + model . pair_preprocessor_ . col_bin_edges_ [ feature_group [ 0 ]] . tolist () + [ np . inf ] ) row_index = list ( pd . IntervalIndex . from_tuples ( list ( zip ( left_mapping [: - 1 ], left_mapping [ 1 :])) ) ) else : left_mapping = model . preprocessor_ . col_mapping_ [ feature_group [ 0 ]] row_index = list ( left_mapping . keys ()) if info_config [ \"feature_type\" ][ 1 ] == \"continuous\" : right_mapping = ( [ - np . inf ] + model . pair_preprocessor_ . col_bin_edges_ [ feature_group [ 1 ]] . tolist () + [ np . inf ] ) col_index = list ( pd . IntervalIndex . from_tuples ( list ( zip ( right_mapping [: - 1 ], right_mapping [ 1 :])) ) ) else : right_mapping = model . preprocessor_ . col_mapping_ [ feature_group [ 1 ]] col_index = list ( right_mapping . keys ()) info_config [ \"column_mapping\" ] = [ left_mapping , right_mapping ] info_config [ \"scores\" ] = model_global . data ( feature_index )[ \"scores\" ] info_config [ \"table\" ] = pd . DataFrame ( info_config [ \"scores\" ], columns = col_index , index = row_index ) else : raise ValueError ( f \"feature type { feature_type } is not supported.\" ) model_config . append ( info_config ) return model_config def bin_mapper ( X , config ): \"\"\" Builds bins in a generic way, a wrapper around pandas cut with some extensions - supports table lookups. If mapping is not provided, will be converted to a one-hot encoded version. bins is a list of bins, columns is a list of columns... Usage: bins = [ [1,2,3], [3,4,5] ] columns = [0, 1] mapping = np.arange(16).reshape(4,4) bin_mapper(np.arange(20).reshape(10, 2), bins, columns, mapping) \"\"\" output = [] if type ( X ) is pd . DataFrame : X_ = X [ config [ 'column_name' ]] else : X_ = X [:, config [ 'column_index' ]] if config [ 'feature_type' ] == 'continuous' : out_ = pd . cut ( X_ , config [ ']) elif config [ 'feature_type' ] == 'categorical' : else : for b , c in zip ( bins , columns ): X_ = X [:, c ] intervals = [ - np . inf ] + b + [ np . inf ] out_ = pd . cut ( X_ , intervals , labels = False ) if mapping is None : out_ = pd . get_dummies ( out_ ) output . append ( out_ ) if mapping is not None : output = np . stack ( output , 1 ) return np . apply_along_axis ( lambda x : mapping [ tuple ( x )], axis = 1 , arr = output ) . reshape ( - 1 , 1 ) return np . stack ( output , 1 ) Despite the short length, it is much more manageable from a production standpoint. Then our code using the EBM can be simplified and decoupled. TODO: Align with ebm2onnx/convert.py to create interfaces import pandas as pd from sklearn.model_selection import train_test_split from sklearn import datasets import numpy as np from interpret.glassbox import ExplainableBoostingRegressor X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) ebm = ExplainableBoostingRegressor ( random_state = seed ) ebm . fit ( X_train , y_train ) ebm_config = build_config ( ebm ) output = np . hstack ([ bin_mapper ( X_train , bins = x [ 'column_mapping' ], columns = x [ 'column_index' ], mapping = x [ 'scores' ]) for x in ebm_config ]) # shape (None, 20) This approach is easier to work with and does not depend on the interpret library! Furthermore, it can be pickled with relative ease without package management. By using the mapping it will generate a 1-D learned embedding in the tabular setting without deep learning. This approach can also be used to extend categorical variables to have a similar encoding scheme without one-hot encoding. This would transform a categorical column to a learned non-linear representation. This becomes a direct value-mapping problem instead. This approach can even be used to determine mappings of interaction variables without exploding the underlying dimensional space without using constructs like \"feature cross\". The issue with feature cross or the PolynomialFeatures creation from scikit-learn leads to an exponential increase in variables which makes it difficult to optimise without greedy search to limit the number of features in their model.","title":"Finding Rules from Discretized Buckets"},{"location":"2.%20Data%20Representation/01-finding-rules/#building-discretized-features","text":"Using scikit-learn we can build discretized features quickly and effectively. from sklearn.model_selection import train_test_split from sklearn import datasets from sklearn.preprocessing import KBinsDiscretizer X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) kbins = KBinsDiscretizer ( n_bins = 20 ) kbins . fit ( X_train , y_train ) output = kbins . transform ( X_train ) # shape: (None, 169)","title":"Building Discretized Features"},{"location":"2.%20Data%20Representation/01-finding-rules/#optimal-using-feature-selection","text":"Even in trivial datasets, the number of features increases exponentially, we can aggressively reduce the feature space, even when taking into account. A common approach is to expand the feature space and add a feature selection component in a pipeline. from sklearn.model_selection import train_test_split from sklearn import datasets , svm from sklearn.preprocessing import KBinsDiscretizer , PolynomialFeatures from sklearn.pipeline import make_pipeline from sklearn.feature_selection import SelectFromModel X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) feature_pipeline = make_pipeline ( PolynomialFeatures ( degree = 2 , interaction_only = True ), KBinsDiscretizer ( n_bins = 256 , strategy = 'uniform' ), SelectFromModel ( svm . LinearSVC ( penalty = \"l1\" , dual = False ), max_features = 512 ) # without this we'll have over 2,000 features ) feature_pipeline . fit ( X_train , y_train ) output = feature_pipeline . transform ( X_train ) # shape: (None, 512) This approach can learn to select features based on linear assumptions. For more background on feature selection in the online setting, you can read more on Grafting.","title":"Optimal using Feature Selection"},{"location":"2.%20Data%20Representation/01-finding-rules/#bonus-optimal-discretized-buckets-using-rulefit","text":"Approaches such as RuleFit will automatically find candidate features as part of its model building process. As it automatically determines interactions and optimises the feature space. To begin we need to install the interpret package which includes wrappers for RuleFit and the official implementation for Explainable Boosting Machines. import pandas as pd from sklearn.model_selection import train_test_split from sklearn.base import TransformerMixin import numpy as np from interpret.glassbox import ExplainableBoostingRegressor from interpret import show X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) ebm = ExplainableBoostingRegressor ( random_state = seed ) ebm . fit ( X_train , y_train ) ebm_global = ebm . explain_global () graph_data = ebm_global . data ( 0 ) # contains information on the feature and confidence interval # for features which use interactions, it looks in all subsets and performs a table lookup (not implemented) class BoostedRegressorBinsDiscretizer ( TransformerMixin ): def __init__ ( self , merge_bins = True , ebm_config = {}): self . ebm_config = ebm_config self . ebm = None self . merge_bins = merge_bins def fit ( self , X , y , ** kwargs ): # ..omitted.. def transform ( self , X ): # ..omitted.. bbd = BoostedRegressorBinsDiscretizer () bbd . fit ( X_train , y_train ) bbd . transform ( X_train ) This constructs a \"smarter\" binning approach than doing it unsupervised (or is a bit bitter than performing it unsupervised followed by feature selection). By performing a table-based lookup when considering interaction terms is the same as constructing a learned embedding over a particular space. This is an efficient way (albeit offline) construction which can help determine non-linearities for used with a linear machine learning model (e.g. as used in Explainable Boosting Machines). Should we use this code for production? Personally, a custom transformers which wraps around an existing model is a bit risky, as having custom code means needing to maintain custom packages. Perhaps building custom pipelines is preferable rather than ephemeral wrappers which are not officially supported. Building pipelines with appropriate metadata interfaces is probably preferable in the longer term. An improved interface could be: import pandas as pd import numpy as np def build_config ( model ): \"\"\" See reference: https://github.com/interpretml/ebm2onnx/blob/master/ebm2onnx/convert.py \"\"\" model_config = [] model_global = model . explain_global () for feature_index in range ( len ( model . feature_names )): feature_name = model . feature_names [ feature_index ] feature_type = model . feature_types [ feature_index ] feature_group = model . feature_groups_ [ feature_index ] info_config = {} if feature_type == \"continuous\" : info_config [ \"feature_type\" ] = feature_type info_config [ \"column_name\" ] = [ feature_name ] info_config [ \"column_index\" ] = [ feature_group [ 0 ]] info_config [ \"column_mapping\" ] = ( [ - np . inf ] + list ( model . preprocessor_ . col_bin_edges_ [ feature_group [ 0 ]]) + [ np . inf ] ) info_config [ \"scores\" ] = model . additive_terms_ [ feature_index ][ 1 :] # print(len(list(pd.IntervalIndex.from_tuples(list(zip(info_config[\"column_mapping\"][0][:-1], info_config[\"column_mapping\"][0][1:]))))), # len(info_config['scores'])) info_config [ \"table\" ] = pd . DataFrame ( { \"interval\" : list ( pd . IntervalIndex . from_tuples ( list ( zip ( info_config [ \"column_mapping\" ][ 0 ][: - 1 ], info_config [ \"column_mapping\" ][ 0 ][ 1 :], ) ) ) ), \"scores\" : info_config [ \"scores\" ], } ) elif feature_type == \"categorical\" : info_config [ \"feature_type\" ] = feature_type info_config [ \"column_name\" ] = [ feature_name ] info_config [ \"column_index\" ] = [ feature_group [ 0 ]] info_config [ \"column_mapping\" ] = model . preprocessor_ . col_mapping_ [ feature_group [ 0 ] ] info_config [ \"scores\" ] = model . additive_terms_ [ feature_index ] row_index = list ( info_config [ \"column_mapping\" ] . keys ()) dummy_index = \" \" while dummy_index in row_index : dummy_index += \" \" row_index = [ dummy_index ] + row_index info_config [ \"table\" ] = pd . DataFrame ( { \"categories\" : row_index , \"scores\" : info_config [ \"scores\" ]} ) elif feature_type == \"interaction\" : # left part right part? I think using range is harder to read - maybe. info_config [ \"feature_type\" ] = [ model . feature_types [ idx ] for idx in feature_group ] info_config [ \"column_name\" ] = [ model . preprocessor_ . feature_names [ idx ] for idx in feature_group ] info_config [ \"column_index\" ] = list ( feature_group ) if info_config [ \"feature_type\" ][ 0 ] == \"continuous\" : left_mapping = ( [ - np . inf ] + model . pair_preprocessor_ . col_bin_edges_ [ feature_group [ 0 ]] . tolist () + [ np . inf ] ) row_index = list ( pd . IntervalIndex . from_tuples ( list ( zip ( left_mapping [: - 1 ], left_mapping [ 1 :])) ) ) else : left_mapping = model . preprocessor_ . col_mapping_ [ feature_group [ 0 ]] row_index = list ( left_mapping . keys ()) if info_config [ \"feature_type\" ][ 1 ] == \"continuous\" : right_mapping = ( [ - np . inf ] + model . pair_preprocessor_ . col_bin_edges_ [ feature_group [ 1 ]] . tolist () + [ np . inf ] ) col_index = list ( pd . IntervalIndex . from_tuples ( list ( zip ( right_mapping [: - 1 ], right_mapping [ 1 :])) ) ) else : right_mapping = model . preprocessor_ . col_mapping_ [ feature_group [ 1 ]] col_index = list ( right_mapping . keys ()) info_config [ \"column_mapping\" ] = [ left_mapping , right_mapping ] info_config [ \"scores\" ] = model_global . data ( feature_index )[ \"scores\" ] info_config [ \"table\" ] = pd . DataFrame ( info_config [ \"scores\" ], columns = col_index , index = row_index ) else : raise ValueError ( f \"feature type { feature_type } is not supported.\" ) model_config . append ( info_config ) return model_config def bin_mapper ( X , config ): \"\"\" Builds bins in a generic way, a wrapper around pandas cut with some extensions - supports table lookups. If mapping is not provided, will be converted to a one-hot encoded version. bins is a list of bins, columns is a list of columns... Usage: bins = [ [1,2,3], [3,4,5] ] columns = [0, 1] mapping = np.arange(16).reshape(4,4) bin_mapper(np.arange(20).reshape(10, 2), bins, columns, mapping) \"\"\" output = [] if type ( X ) is pd . DataFrame : X_ = X [ config [ 'column_name' ]] else : X_ = X [:, config [ 'column_index' ]] if config [ 'feature_type' ] == 'continuous' : out_ = pd . cut ( X_ , config [ ']) elif config [ 'feature_type' ] == 'categorical' : else : for b , c in zip ( bins , columns ): X_ = X [:, c ] intervals = [ - np . inf ] + b + [ np . inf ] out_ = pd . cut ( X_ , intervals , labels = False ) if mapping is None : out_ = pd . get_dummies ( out_ ) output . append ( out_ ) if mapping is not None : output = np . stack ( output , 1 ) return np . apply_along_axis ( lambda x : mapping [ tuple ( x )], axis = 1 , arr = output ) . reshape ( - 1 , 1 ) return np . stack ( output , 1 ) Despite the short length, it is much more manageable from a production standpoint. Then our code using the EBM can be simplified and decoupled. TODO: Align with ebm2onnx/convert.py to create interfaces import pandas as pd from sklearn.model_selection import train_test_split from sklearn import datasets import numpy as np from interpret.glassbox import ExplainableBoostingRegressor X , y = datasets . load_diabetes ( return_X_y = True ) seed = 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = seed ) ebm = ExplainableBoostingRegressor ( random_state = seed ) ebm . fit ( X_train , y_train ) ebm_config = build_config ( ebm ) output = np . hstack ([ bin_mapper ( X_train , bins = x [ 'column_mapping' ], columns = x [ 'column_index' ], mapping = x [ 'scores' ]) for x in ebm_config ]) # shape (None, 20) This approach is easier to work with and does not depend on the interpret library! Furthermore, it can be pickled with relative ease without package management. By using the mapping it will generate a 1-D learned embedding in the tabular setting without deep learning. This approach can also be used to extend categorical variables to have a similar encoding scheme without one-hot encoding. This would transform a categorical column to a learned non-linear representation. This becomes a direct value-mapping problem instead. This approach can even be used to determine mappings of interaction variables without exploding the underlying dimensional space without using constructs like \"feature cross\". The issue with feature cross or the PolynomialFeatures creation from scikit-learn leads to an exponential increase in variables which makes it difficult to optimise without greedy search to limit the number of features in their model.","title":"(Bonus) Optimal Discretized Buckets using RuleFit"},{"location":"2.%20Data%20Representation/02-transfer-learning/","text":"Another approach is to use transfer learning for build machine learning models. In low resource settings, we typically do not want to go down the fine tuning route. Its difficult to get right and not guarenteed to get the results we want. Instead, we can use the embedding layers as inputs for another machine learning model. One simple approach is to use approximate nearest neighbours over the pre-built embeddings as the basis to infer concepts to a new domain. Variation: using annoy from sklearn.base import ClassifierMixin from sklearn.neighbors import KNeighborsClassifier from annoy import AnnoyIndex # pip install annoy import pandas as pd import numpy as np from sklearn import datasets class FastANN ( ClassifierMixin ): def __init__ ( self , n_neighbors = 5 , n_trees = 100 , metric = 'angular' ): self . n_neighbors = n_neighbors self . n_trees = n_trees self . metric = metric def fit ( self , X , y ): self . y = y self . ss = pd . Series ( y ) . value_counts () * 0 self . ann = AnnoyIndex ( X . shape [ 1 ], self . metric ) for i in range ( X . shape [ 0 ]): self . ann . add_item ( i , X [ i , :]) self . ann . build ( self . n_trees ) return self def predict_single_proba ( self , v ): indx = self . ann . get_nns_by_vector ( v , self . n_neighbors ) ss = ( self . ss + pd . Series ( y [ indx ]) . value_counts ( normalize = True )) . fillna ( 0 ) . values return ss def predict_ ( self , X ): pred = [] for i in range ( X . shape [ 0 ]): pred . append ( self . predict_single_proba ( X [ i , :])) return np . stack ( pred , 0 ) def predict_proba ( self , X ): return self . predict_ ( X ) def predict ( self , X ): return np . argmax ( self . predict_ ( X ), axis = 1 ) X , y = datasets . load_breast_cancer ( return_X_y = True ) model = FastANN ( n_neighbors = 5 , n_trees = 10 ) model . fit ( X , y ) model . score ( X , y ) model2 = KNeighborsClassifier () model2 . fit ( X , y ) model2 . score ( X , y ) Variation: using faiss from sklearn.base import ClassifierMixin from sklearn.neighbors import KNeighborsClassifier import faiss # conda install -c pytorch faiss-cpu import pandas as pd import numpy as np from sklearn import datasets class FastANN ( ClassifierMixin ): def __init__ ( self , n_neighbors = 5 , n_trees = 100 , metric = 'angular' ): self . n_neighbors = n_neighbors self . n_trees = n_trees self . metric = metric def fit ( self , X , y ): X = X . astype ( np . float32 ) self . y = y self . ss = pd . Series ( y ) . value_counts () * 0 self . ann = faiss . IndexFlatL2 ( X . shape [ 1 ]) self . ann . add ( X ) return self def predict_single_proba ( self , v ): _ , indx = self . ann . search ( v , self . n_neighbors ) ss = ( self . ss + pd . Series ( y [ indx . flatten ()]) . value_counts ( normalize = True )) . fillna ( 0 ) . values return ss def predict_ ( self , X ): X = X . astype ( np . float32 ) pred = [] for i in range ( X . shape [ 0 ]): pred . append ( self . predict_single_proba ( X [[ i ], :])) return np . stack ( pred , 0 ) def predict_proba ( self , X ): return self . predict_ ( X ) def predict ( self , X ): return np . argmax ( self . predict_ ( X ), axis = 1 ) X , y = datasets . load_breast_cancer ( return_X_y = True ) model = FastANN ( n_neighbors = 5 , n_trees = 10 ) model . fit ( X , y ) model . score ( X , y ) model2 = KNeighborsClassifier () model2 . fit ( X , y ) model2 . score ( X , y ) Although the performance in both variations is worse than using the built-in k-nearest neighbors classifier, it is an order of magnitude faster on larger datasets. This kind of fine-tuning can be used to build and deploy models in resource constrained settings.","title":"Transfer Learning with Embeddings"},{"location":"3.%20Data%20Engineering/01-pipelines-in-dagster/","text":"Building feature pipelines is important as part of production. Here I'll recommend approaches which can enable you to test deployments locally and also in a managed cloud environment. There are several options here, we'll recommend using: dagster luigi In my (albeit) limited experience Airflow suffers from the challenges surrounding ability to ensure local development workflows can run well in the cloud. We can make use of Kubernetes runners to ensure that workflows can operate in the expected manner. Using dagster and luigi also have straightforward processes and can both be run without a daemon or server for testing before using the centralized runner. This enables a split between development and production!","title":"Managing Pipelines"}]}